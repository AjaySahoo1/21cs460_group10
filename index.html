

<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>CS460/600 Project</title>
    
    <link href="http://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
    
    <style> 
    figcaption {
  
  color: blue;
  font-style: italic;
  padding: 2px;
  text-align: center;
}
</style>

<style>
* {
  box-sizing: border-box;
}

.row {
  display: flex;
}
h1 {
  text-align: center;
}

/* Create three equal columns that sits next to each other */
.column {
  flex: 33.33%;
  padding: 5px;
}
div.box {
  box-sizing: border-box;
  width: 100%;
  border: 5px blue;
  
}
</style>
</head>



<body text="Black">
    <div class="container">
        <h1><center><pre>CS460 Machine Learning</pre></center> </h1>
      <div class="box"><b>Ajaya Saipriya Sahoo, 4th year Int Msc, School of Mathematical Sciences, NISER <br>
      Prateek Kumar Murmu, 4th year Int Msc, School of Mathematical Sciences, NISER<br>
Project Group No. 10</b></div> 
      <hr/>
    
      
      <div class="box"><b>
      Link to access files:</b> <a href="https://github.com/AjaySahoo1/21cs460_group10">Link</a> </div>

      <br>
      <br>



        <h1> <u>Stock Prediction using SVR</u></h1>
        <br>
        <br>
        <br>

        <h2>Problem Statement</h2>
<p>We all know the value of money. Everyone wants to earn to live a stable life. But we also want to become rich with low efforts and great advantages. We all have some kind of wish list in our mind and we need a lot of money to fulfill
those desires. Stock market is one of the best platform for this. But again there are risks which we don't want. Or do you want to take risk and lose your money? No, right? Even though there are risks one can forecast the stocks 
by visualising the past stock values and some statistical factors.</p>

<p>In this project we are trying to plots of financial data of a specific company by using tabular data provided by "yfinance" python library. Again we will create a Support Vector Regression(SVR) model to predict upcoming stock 
prices. This will reduce the risk factor for the investors.</p>

<hr/>

<h2>Datasets and Idea</h2>
<p>We are going to use Yahoo Finanace, Quandle and if possible some other platform from which we can get sufficient dataset for stock prediction. We will create a SVR model. We will split the data set into training and testing data.
After training the model we will check the performance of the model by using different metrics(e.g mean square error, mean absolute error). We have already studied about the Support Vector Machine for Linear Regression and SVR uses same 
principle as SVM.

<hr/>

<h2>Work Division</h2>
<p>No team is a perfect team without teamwork and coordination. We will discuss everything about our work. But roughly saying, Ajaya is going to give the 1st and 2nd Presentation, create slides and Prateek is going to
present the last presentation, provide the summary of all theories for the report. Creating ML model using python and handling website for reports will come under joint task. </p>

<hr/>

<h2> Mid-way work and Expecting results</h2>
<p>Since we are taking support vector machine as our baseline, our main focus is to understand how the SVR algorithm works theoratically. After that we will try to build the model for it, test the performance of trained model, forecast 
the stock for a certain date by plotting a graph between "exponential moving average vs date". At the latter stage we are planning to compare the result with the result obtained by other algorithm for example Long Short-Term Memory
(LSTM) model.</p>
<p> There are some difficulties we might face such as SVR model may not work for large data sets, choosing a good kernel and fine-tuning the hyperparameters are not easy task. So the final goal of the project to bulld a model that might
overcome some of these obstacles.</p>

<hr/>

<body class="c16"><p class="c0"><span class="c14"><b>1.Theoretical</b></span><span class="c23 c14 c20"><b>&nbsp;Analysis (Using different sources/papers)</b></span></p><p class="c0"><span class="c17 c14"><u>Support Vector Regression (SVR):</u></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">A Support Vector Machine (SVM) is a discriminative classifier that is formally defined by the separating hyperplane. This algorithm outputs an optimal hyperplane which categorizes new examples. It is considered to be one of the most suitable algorithms available for time series prediction. This algorithm can be used for both classification and regression problems.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">SVM involves plotting data as points in the multidimensional space. The dimensions represent the attributes (or parameters) of our given data. This algorithm sets a boundary on the dataset called hyperplane. The hyperplane separates our data points into separate classes. To find the best hyperplane (or decision boundary) is our goal. And by best we mean, the decision boundary would be associated with a maximum marginal distance. </span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Let &nbsp;&micro; be some unknown data point and w be the vector which is perpendicular to the hyperplane. So, now our decision rule will be,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><img src="https://drive.google.com/uc?export=view&id=17NgMhNLLz6RaNQOqsFnzmb1sn1PFAL3Q"><span class="c3">+ b</span><img src="https://drive.google.com/uc?export=view&id=1zsS_H5XQfv7c9HYlWDN8TUrx5-xEZZPD"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (1)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Width of the margin of the hyperplane must be maximized to get a good hyperplane.</span></p><p class="c0"><span class="c3">Width = [2 / ||</span><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><span class="c1">||] ................................................... (2)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Max of Width = Max [2 / ||</span><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><span class="c1">||] ..........................................(3)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Applying Lagrange&#39;s multiplier as,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">L = </span><img src="https://drive.google.com/uc?export=view&id=1H3w3QA0zBD_h3ihgrZeB-cIlkfPqDLvn"><span class="c3">||</span><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><span class="c3">||&sup2; - </span><img src="https://drive.google.com/uc?export=view&id=1RNX5Eo8WjCEZAI1U1M0c-Mfxq2H1Qn6d"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (4)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">where , </span><img src="https://drive.google.com/uc?export=view&id=1gvKPwkWBFhkxE6_u0h4tInQsLrtJUQts"><span class="c1">&rsquo;s are the class classifications.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">L = </span><img src="https://drive.google.com/uc?export=view&id=178EoTdWdIEwERhrG2d_fMbHUAEyXECF6"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (5)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">By finding the extremum of the above Lagrangian L, we get our desired result.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now our decision rule will be,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1Miq-Q6ef44LfJtuuk0Ix9F9Dy2AQu7C7"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (6)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">If we get a dataset which is non-linear in the current dimensional space, we can map them to a new space with greater dimension than before.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">The hyperplane in our new dimension is given by the equation,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1aDA2uxLcBhOHRyvic9E0ok6QJVx6XQfK"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (7)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">And the hyperplane must satisfy,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=12st4zX1rwVUo_q9Ih7DDKe4P3MzL5NsI"><span class="c3">&nbsp;for positive samples, i.e. when </span><img src="https://drive.google.com/uc?export=view&id=1wFYQxlHGjyy24I_WM31I5UN1hdC-OZIB"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;..(8)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1ymtLRDjPrmnijHvFMiFfQXa3lsHucRh9"><span class="c3">&nbsp;for the negative samples, i.e. when </span><img src="https://drive.google.com/uc?export=view&id=13WM154WMsF5_eD8r-GHqBdOoeJkUGv5w"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.(9)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Here </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c1">&nbsp;maps our independent values to the new space with greater dimension in which our dataset turns to be linearly separable.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">To summarize the above two inequalities we can write,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1NhkeU8aFSoMpz5brn37zy9X5c3HqNme2"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (10)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">In real world problems it can happen that even in the new space the dataset is not linearly separable. To counter this problem we introduce a variable, </span><img src="https://drive.google.com/uc?export=view&id=1ZrTBM3gXm0rPWC7zEBiB_sR9XwVIqBNR"><span class="c1">&nbsp;as a tolerance margin in the classification thresholds, making the classifier more flexible in accepting possible errors. Now &nbsp;the hyperplane condition in Eq. (10) becomes Eq. (11), and the problem of finding the optimal hyperplane becomes a convex optimization problem given by Eq. (12). In this equation, C is the adjustment parameter for the edge of the hyperplane with the smallest possible misclassification, under the conditions of Eq. (11).</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1iKJdmpRsRLyoZEzFsajA2ype9uJdoEVq"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (11)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1N43EO4bjU6qM6wVZtQ_rl8nw-FIMY_zB"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.... (12)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Now we come to SVR. It uses principles similar to SVM, but the response variable is a continuous value </span><img src="https://drive.google.com/uc?export=view&id=1nsgwJ7ntwaTywFlWpcRz5F6H5VN1HMkl"><span class="c3">. Instead of seeking for the hyperplane in Eq. (11), SVR seeks the linear regression function, given by Eq. (13). To achieve this, a threshold error &epsilon; is defined to be minimized in the expression in Equation (14). This expression is called the &epsilon;-insensitivity loss error function. The SVR regression process therefore seeks to minimize &epsilon; in Eq. (14) and </span><img src="https://drive.google.com/uc?export=view&id=1u-UagvNlkUgl374MoVofWmlGrgQDzL91"><span class="c1">&nbsp;in the expression of R, defined in Eq. (15).</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1zJC5Aj3on5D7vjG4CyOGxH2e0UvGj6OM"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (13)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1ecBzHDzK7jJm6do_bbVhiXNstCVdyXUh"><span class="c1">&nbsp;&hellip;&hellip;. (14)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1AjOS8lEa2ne13kff9EIDFTZVI8gTWfRC"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (15)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Again introducing the tolerance variables here as well, &nbsp;defining </span><img src="https://drive.google.com/uc?export=view&id=1jnpWCuITmMwDol8rUpQX1jtKECFfl0Ro"><span class="c3">&nbsp;as the value in excess of &epsilon; and </span><img src="https://drive.google.com/uc?export=view&id=10ZrfxIrOYcGDSFKZ8tmJjOlgY9royWmQ"><span class="c3">&nbsp;to limit the value to the regression target. Thus, the minimization of Eq. (15) becomes Eq. (16), under the conditions of Eqs. (17) and (18) for </span><img src="https://drive.google.com/uc?export=view&id=1y4okkqF3oWJPWv-D-4XZdFunohhGa2l_"><span class="c3">&nbsp;and </span><img src="https://drive.google.com/uc?export=view&id=1vOM-S_UO9DiJt-vUZ9QWLV7Fk9s1mQ2F"><span class="c1">.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1nBcCQOHNEfwoO3OPo38EdlXalcCSlC84"><img src="https://drive.google.com/uc?export=view&id=1y4okkqF3oWJPWv-D-4XZdFunohhGa2l_"><span class="c3">+</span><img src="https://drive.google.com/uc?export=view&id=1OeK2RNrVv-5B8DihiTPIanmEHiXKtwHa"><span class="c1">) ........................................................... (16)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1uZ_8__J83OPK3tc4A3Sx4hP8PfnUxgYw"><img src="https://drive.google.com/uc?export=view&id=1y4okkqF3oWJPWv-D-4XZdFunohhGa2l_"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (17)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1UOZK54Ld1sIWMWFJCKT8GJrV6IKSC6Ez"><img src="https://drive.google.com/uc?export=view&id=1OwZR8A2qEQOiI8ZYnxTm0TK0hxIw0h3m"><img src="https://drive.google.com/uc?export=view&id=1OeK2RNrVv-5B8DihiTPIanmEHiXKtwHa"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (18)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now let&rsquo;s focus on the Kernel Function now. It is,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">&nbsp;</span><img src="https://drive.google.com/uc?export=view&id=1SrkAaS6ldHa9G3qjKuIoUbN-vOvTwvye"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (19)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">So, it is the dot product of the </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c1">&nbsp;images of the vectors in our current space.</span></p><p class="c0"><span class="c3">The perks of kernels is that we can get the dot products of the </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c3">images without even knowing anything about the map </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c1">.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">There are mainly four types of kernel function in the SVM algorithm, namely, linear, radial basis function (RBF) Eq.(20), polynomial Eq.(21) and sigmoid function Eq.(22). In this project we have used RBF, polynomial and sigmoid functions and have compared the results.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1c7XsX8G2X6nFBi7Et4zzZtgzsg0OAqcz"><span class="c1">where gamma is a parameter &hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (20)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1oprInpHu6IDdaVZWs1jy3bJupZ4YSEOH"><span class="c1">&nbsp;where d and r are parameters &hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (21)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1icL7ETHxhDrP5ofLn_yCSzWB_DhWCHtT"><span class="c3">&nbsp;where </span><img src="https://drive.google.com/uc?export=view&id=1uMt_813TkEniUq14K-IIQegOdV5qZ6lp"><span class="c1">&nbsp;and r are parameters &hellip;&hellip;&hellip;&hellip;&hellip;... (22)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">In the RBF kernel, </span><img src="https://drive.google.com/uc?export=view&id=1gFxtmppjKtnxNi6m2gi7234xK4_ifyg0"><span class="c3">&nbsp;is the squared Euclidean distance between two feature vector and </span><img src="https://drive.google.com/uc?export=view&id=1uMt_813TkEniUq14K-IIQegOdV5qZ6lp"><span class="c1">is a parameter which determines how much influence a single training data point has. RBF kernel is a function whose value depends on the distance from the origin or from some point.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">In the polynomial kernel, d is the degree of the kernel and r is a constant term. Here, we simply calculate the dot product by increasing the power of the kernel.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">An interesting fact to be noted is that the shape of the kernel function directly influences the values obtained by the SVR. Similarly, the constant c in Eq. (16) and the parameters </span><img src="https://drive.google.com/uc?export=view&id=1uMt_813TkEniUq14K-IIQegOdV5qZ6lp"><span class="c1">&nbsp;and d in Eqs. (21) and (22) should be optimized.</span></p><p class="c0 c4"><span class="c1"></span></p>

<p class="c0"><span class="c14"><b>2.Experiment and Results:</b></span></p><p class="c0 c4"><span class="c23 c14 c20"></span></p><ol class="c5 lst-kix_list_5-0 start" start="1"><li class="c0 c8 li-bullet-0"><span class="c17 c3"><u>Data Set</u></span><span class="c1">: We have </span><span class="c3">taken the Uniqlo</span><span class="c1">&nbsp;(Fast Retailing ) in Tokyo Stock Exchange dataset available on Kaggle. The dataset contains data of 5 years i.e 2012-2016. This is how our data looks like.</span></li></ol><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 480.00px; height: 164.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1YJMOO28ci-tcAeiBRFJA2rWaQ19YqUNh" style="width: 480.00px; height: 164.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c4"><span class="c1"></span></p><ol class="c5 lst-kix_list_5-0" start="2"><li class="c0 c8 li-bullet-0"><span class="c3 c17"><u>Preprocessing Data and Data Analysis:</u></span><span class="c1">&nbsp;We only need to predict the &lsquo;Close Price&rsquo; with respect to Date. After checking whether there </span><span class="c3">exists a null</span><span class="c1">&nbsp;value in the date column or not, we changed the data type of the Date column from string to date-time. Then we compared the month-wise opening and closing price of actual data.</span></li></ol><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 676.16px; height: 372.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1kymAJ4Nie23eU4tqWVhXjhKJq2tYowtp" style="width: 676.16px; height: 372.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c0"><span class="c1">Using plotly we got a figure of opening and close price of actual data vs Date.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 658.63px; height: 348.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1RXNZLaMy0YUk4RA0gD-KnJTj44aGYzgS" style="width: 658.63px; height: 348.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c5 lst-kix_list_5-0" start="3"><li class="c0 c8 li-bullet-0"><span class="c17 c3"><u>Close Price Prediction:</u></span><span class="c1">&nbsp;We made a separate data frame with Date and Close as columns and took a copy of the same for further use. After normalizing the feature(close value) into range &nbsp;0 to 1, we split the data into 70% training data and 30% testing data. Using different kernels (RBF, Sigmoid, Polynomial(degree=2)) of SVR from Sci-kit Learn we got the following graphs as result.</span></li></ol><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 636.00px; height: 321.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1tKCm5A4HFme0GNagtYh6TJkIqHheTEFX" style="width: 636.00px; height: 321.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 644.00px; height: 280.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1x5frJj1W_SLv1uP4JYM7jSCN9cAxZVYo" style="width: 644.00px; height: 280.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 648.00px; height: 320.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1GacXc6fLSLLoYnt-bTti05iiFWIfHsja" style="width: 648.00px; height: 320.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3 c20">After getting the model we will predict the close price using different kernels. Here it is shown </span><span class="c3">for the next</span><span class="c1">&nbsp;30 days.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 649.24px; height: 315.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1vjFKilU0DZHasB95BQp2adn8tGJUioEk" style="width: 649.24px; height: 315.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">

<p class="c0"><span class="c1">Now, the question is which kernel we should use for better result. Let&rsquo;s look into a statistical measure called the R-squared.</span></p><p class="c0"><span class="c3">It </span><span class="c3 c21">represents the proportion of the variance for a dependent variable that&#39;s explained by an independent variable or variables in a regression model. </span><span class="c1">&nbsp;We have calculated the R^2 value(R) for each kernel. We know that for R=1, it is considered as a best model and for R=0 or &lt; 0 ; it is considered as a worse model.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 659.00px; height: 335.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1x1nRqYBk5m5r5HmZXdEx83YTevscTv3F" style="width: 659.00px; height: 335.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">Considering the above measure, we can clearly see that &lsquo;Sigmoid&rsquo; is a very bad kernel option for our dataset. That&rsquo;s why we haven&rsquo;t considered it for the &lsquo;30 days prediction&rsquo;.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now, comparing the above statistical measures for RBF and Polynomial kernel, we can conclude that for our experiment RBF kernel is the best model.</span></p><p class="c0 c4"><span class="c1"></span></p>



<iframe src="https://onedrive.live.com/embed?cid=ECFD7DE26A6B7D51&amp;resid=ECFD7DE26A6B7D51%21781&amp;authkey=AHUszNk1NEWEH4U&amp;em=2&amp;wdAr=1.7777777777777777" width="1186px" height="691px" frameborder="0">This is an embedded <a target="_blank" href="https://office.com">Microsoft Office</a> presentation, powered by <a target="_blank" href="https://office.com/webapps">Office</a>.</iframe>


<p class=MsoNormal style='text-align:justify'><b><u><span lang=EN-GB
style='font-size:16.0pt;line-height:107%;font-family:"Times New Roman",serif'>Final
Report:</span></u></b></p>

<p class=MsoNormal style='text-align:justify'><u><span lang=EN-GB
style='font-size:16.0pt;line-height:107%;font-family:"Times New Roman",serif'>Review:</span></u></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>Let’s
first go through what we have done till now(briefly). We have described how the
SVR algorithm works by giving a theoretical explanation. Further we moved into
our model. We created three different models using three different kernels. The
kernels we have used are RBF (Radial Basis Function), Polynomial (degree= 2)
and Sigmoid. The graphical representation of Close Price vs Date was presented
for these different models.</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>We
used a specific set of values for the parameters of RBF kernel. The values for
C and gamma that are used are 100 and 0.1 respectively. For this model we got
almost around 93% accuracy. The polynomial kernel model gave us almost 74%
accuracy. [Here the term ‘accuracy’ is used for percentage value of R-squared
score. For example, for model-1 if r-squared value is 0.78 we are saying that
the model is 78% accurate.]</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>We
created another model for RBF kernel with default parameters i.e., C=1 and
gamma=1. For this new model we got almost 83% accuracy. Here we present the
graph for two different RBF models.</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>Fig.
RBF model with default parameters</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB><img width=611
height=301 id="Picture 1602236" src="https://drive.google.com/uc?export=view&id=1vlt0bG37uoQqcNDeiMB3XapynPIidxAD"></span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>Fig.
RBF model with C=100 and gamma=0.1</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB><img width=615
height=308 id="Picture 1506084800" src="https://drive.google.com/uc?export=view&id=15P8eKrSF_HS0aqZhumdpHCZ8ZzXDK1Ez"></span></p>

<p class=MsoNormal style='text-align:justify'><u><span lang=EN-GB
style='font-size:16.0pt;line-height:107%;font-family:"Times New Roman",serif'>Ideas
to improve accuracy:</span></u></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>First,
we tried to improve the accuracy of RBF model with default parameters by tuning
the hyperparameters. To find a good set of values for C and gamma, we used Grid
Search method. We took a no. of sets of values of C and gamma as parameters
with 5-fold cross validation. Using Grid search we got the best set of C and
gamma value from those taken values. For this set of values, the RBF model gave
almost 96.5% accuracy.</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>Another
idea is we can create a hybrid model. Instead of searching for an optimal set
of hyperparameters for RBF kernel, we tried to use hybrid model so that we can
get more accuracy than polynomial model. To create a hybrid model, we are
proposing to create a model with mixed kernel function. Let’s say we have ‘A’
as kernel function for RBF and ‘B’ as kernel function for polynomial. We
somehow create a kernel function K = aA+(1-a)B, where ‘a’ is a positive
constant such that 0&lt;a&lt;1. In this way, we can use the mixed kernel and
get benefitted by both RBF and polynomial kernel. We are proposing that the
value of ‘a’ should be taken small so that the impact of RBF will be less.
Unfortunately, we couldn’t provide an experimental proof or any result to
validate the idea due to some issues while running the python code. The last
paper in the reference list gives us some theoretical proof to back this idea.
So, we are providing another idea.</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>Before
discussing the last idea, let’s explain what the polynomial kernel and RBF
kernel do. Polynomial and radial basis function (RBF) kernels have
complementary strengths. Polynomial kernels perform better for extrapolation.
RBF kernels give a better fit in the region covered by the training data Poly
kernel has extremely strong generalizability but weak learning capacity. By
contrast, Gaussian RBF is it has strong learning capacity but weak
generalizability. Based on these concepts, we are proposing an idea of creating
a hybrid model in which first we will train an RBF kernel model with original
training data set. Then we will predict the values when the input is training
data. Now we will create another polynomial kernel model which will be trained
by original x values of training data and predicted y values from above model.
This will be our hybrid model. This model gives us 82% accuracy which is better
than the polynomial kernel. Note: We are simply proposing this idea. We are not
providing any strong theoretical proof for this. But here is the result we got
for this hybrid kernel.</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB><img width=604
height=296 id="Picture 1010999098" src="https://drive.google.com/uc?export=view&id=1US645iqS4ya0MT7t15sI42sU8gx-ZXqe"></span></p>

<p class=MsoNormal style='text-align:justify'><u><span lang=EN-GB
style='font-size:14.0pt;line-height:107%;font-family:"Times New Roman",serif'>Limitations
and Conclusion:</span></u></p>

<p class=MsoListParagraphCxSpFirst style='text-align:justify;text-indent:-.25in'><span
lang=EN-GB style='font-size:14.0pt;line-height:107%;font-family:Symbol'>·<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span lang=EN-GB style='font-size:14.0pt;line-height:107%;
font-family:"Times New Roman",serif'>Finding optimal hyperparameters using Grid
Search is computationally expensive. </span></p>

<p class=MsoListParagraphCxSpMiddle style='text-align:justify;text-indent:-.25in'><span
lang=EN-GB style='font-size:14.0pt;line-height:107%;font-family:Symbol'>·<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span lang=EN-GB style='font-size:14.0pt;line-height:107%;
font-family:"Times New Roman",serif'>We still need a strong theoretical proof
to verify the effectiveness of the Hybrid Model for different datasets Also it
depends on the user how much RBF and polynomial part we need to create a good
hybrid model for the data.</span></p>

<p class=MsoListParagraphCxSpMiddle style='text-align:justify;text-indent:-.25in'><span
lang=EN-GB style='font-size:14.0pt;line-height:107%;font-family:Symbol'>·<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span lang=EN-GB style='font-size:14.0pt;line-height:107%;
font-family:"Times New Roman",serif'>With high prediction accuracy, SVR
implementation is easy and it is robust to outliers. For very large data sets,
it is not suitable. Here as our dataset is small the results might be a
consequence of overfitting.</span></p>

<p class=MsoListParagraphCxSpLast style='text-align:justify;text-indent:-.25in'><span
lang=EN-GB style='font-size:14.0pt;line-height:107%;font-family:Symbol'>·<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><span lang=EN-GB style='font-size:14.0pt;line-height:107%;
font-family:"Times New Roman",serif'>The Hybrid Model-2 gives a good accuracy
of 82%.</span></p>


<p class="c0"><span class="c17 c14"><b>Reference:</b></span></p><ol class="c5 lst-kix_list_3-0 start" start="1"><li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Vanukuru K.S.R, &quot;Stock Market Prediction Using Machine Learning&quot;, Vol. 05 Issue. 10, International Research Journal of Engineering and Technology (IRJET), Oct 2018. </span><span class="c11 c3"><a class="c10" href="https://www.google.com/url?q=http://dx.doi.org/10.13140/RG.2.2.12300.77448&amp;sa=D&amp;source=editors&amp;ust=1634328078768000&amp;usg=AOvVaw2DvZnkKNmVEY3yLLBwS5Kx">Link</a></i></span><span class="c9 c3">. </span></li><li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Henrique B.M, Sobreiro V.A and Kimura H, Stock Price Prediction Using Support Vector Regression on Daily and Up to the Minute Prices, The Journal of Finance and Data Science, vol. 4, no. 3, pp. 183-201, 2018. </span><span class="c11 c3"><a class="c10" href="https://www.google.com/url?q=https://doi.org/10.1016/j.jfds.2018.04.003&amp;sa=D&amp;source=editors&amp;ust=1634328078769000&amp;usg=AOvVaw25SLJBiIBwADiuSTms5Jp3">Link</a></i></span><span class="c3 c9">.</span></li><li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Nanda M.A, Seminar K.B, Nandika D and Maddu A, &ldquo;</span><span class="c3 c12">A Comparison Study of Kernel Functions in the Support Vector Machine and Its Application for Termite Detection</span><span class="c9 c3">&rdquo;, Information (Switzerland), 9(1), 5, Jan 2018. </span><span class="c3 c11"><a class="c10" href="https://www.google.com/url?q=https://doi.org/10.3390/info9010005&amp;sa=D&amp;source=editors&amp;ust=1634328078769000&amp;usg=AOvVaw0bhet3s8-16m_DRNtpdUrc">Link.</a><i></span></li>

<li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Kari T, Gao W, Tuluhong A, Zhang Z and Yaermairmaiti Y, &ldquo;</span><span class="c3 c12">Article Mixed Kernel
Function Support Vector Regression with Genetic Algorithm for Forecasting
Dissolved Gas Content in Power Transformers</span><span class="c9 c3">&rdquo;,Energies 2018 11 9 2437.</span><span class="c3 c11"><a class="c10" href="https://doi.org/10.3390/en11092437">Link.</a><i></span></li>

</ol>
<iframe src="https://onedrive.live.com/embed?cid=ECFD7DE26A6B7D51&amp;resid=ECFD7DE26A6B7D51%21783&amp;authkey=ADsXG5vDi_n_MNo&amp;em=2&amp;wdAr=1.7777777777777777" width="1186px" height="691px" frameborder="0">This is an embedded <a target="_blank" href="https://office.com">Microsoft Office</a> presentation, powered by <a target="_blank" href="https://office.com/webapps">Office</a>.</iframe>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:12.0pt;line-height:107%;font-family:"Times New Roman",serif'>*<u>Remark</u>:
The proposed ideas for hybrid model-1 and 2 may not work for every data sets.
We are simply proposing some ideas without any valid proof that might work,
according to us. Also, these kinds of simple models may not work in real life
stock market prediction as the real stock market depends on a lot of factors.
Thank you.</span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-GB
style='font-size:12.0pt;line-height:107%;font-family:"Times New Roman",serif'>&nbsp;</span></p>


</body></html>
