<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <title>CS460/600 Project</title>
    
    <link href="http://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
    
    <style> 
    figcaption {
  
  color: blue;
  font-style: italic;
  padding: 2px;
  text-align: center;
}
</style>

<style>
* {
  box-sizing: border-box;
}

.row {
  display: flex;
}
h1 {
  text-align: center;
}

/* Create three equal columns that sits next to each other */
.column {
  flex: 33.33%;
  padding: 5px;
}
div.box {
  box-sizing: border-box;
  width: 100%;
  border: 5px blue;
  
}
</style>
</head>



<body text="Black">
    <div class="container">
        <h1><center><pre>CS460 Machine Learning</pre></center> </h1>
      <div class="box"><b>Ajaya Saipriya Sahoo, 4th year Int Msc, School of Mathematical Sciences, NISER <br>
      Prateek Kumar Murmu, 4th year Int Msc, School of Mathematical Sciences, NISER<br>
Project Group No. 10</b></div> 
      <hr/>
    
      
      <div class="box"><b>
      Link to access files:</b> <a href="https://github.com/AjaySahoo1/21cs460_group10">Link</a> </div>

      <br>
      <br>


<body class="c16"><p class="c0"><span class="c14"><b>1.Theoretical</b></span><span class="c23 c14 c20"><b>&nbsp;Analysis (Using different sources/papers)</b></span></p><p class="c0"><span class="c17 c14"><u>Support Vector Regression (SVR):</u></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">A Support Vector Machine (SVM) is a discriminative classifier that is formally defined by the separating hyperplane. This algorithm outputs an optimal hyperplane which categorizes new examples. It is considered to be one of the most suitable algorithms available for time series prediction. This algorithm can be used for both classification and regression problems.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">SVM involves plotting data as points in the multidimensional space. The dimensions represent the attributes (or parameters) of our given data. This algorithm sets a boundary on the dataset called hyperplane. The hyperplane separates our data points into separate classes. To find the best hyperplane (or decision boundary) is our goal. And by best we mean, the decision boundary would be associated with a maximum marginal distance. </span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Let &nbsp;&micro; be some unknown data point and w be the vector which is perpendicular to the hyperplane. So, now our decision rule will be,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><img src="https://drive.google.com/uc?export=view&id=17NgMhNLLz6RaNQOqsFnzmb1sn1PFAL3Q"><span class="c3">+ b</span><img src="https://drive.google.com/uc?export=view&id=1zsS_H5XQfv7c9HYlWDN8TUrx5-xEZZPD"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (1)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Width of the margin of the hyperplane must be maximized to get a good hyperplane.</span></p><p class="c0"><span class="c3">Width = [2 / ||</span><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><span class="c1">||] ................................................... (2)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Max of Width = Max [2 / ||</span><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><span class="c1">||] ..........................................(3)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Applying Lagrange&#39;s multiplier as,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">L = </span><img src="https://drive.google.com/uc?export=view&id=1H3w3QA0zBD_h3ihgrZeB-cIlkfPqDLvn"><span class="c3">||</span><img src="https://drive.google.com/uc?export=view&id=1D5W9P4K73t1BNvWrkzHvg-oA1N9HaCMT"><span class="c3">||&sup2; - </span><img src="https://drive.google.com/uc?export=view&id=1RNX5Eo8WjCEZAI1U1M0c-Mfxq2H1Qn6d"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (4)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">where , </span><img src="https://drive.google.com/uc?export=view&id=1gvKPwkWBFhkxE6_u0h4tInQsLrtJUQts"><span class="c1">&rsquo;s are the class classifications.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">L = </span><img src="https://drive.google.com/uc?export=view&id=178EoTdWdIEwERhrG2d_fMbHUAEyXECF6"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (5)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">By finding the extremum of the above Lagrangian L, we get our desired result.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now our decision rule will be,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1Miq-Q6ef44LfJtuuk0Ix9F9Dy2AQu7C7"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (6)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">If we get a dataset which is non-linear in the current dimensional space, we can map them to a new space with greater dimension than before.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">The hyperplane in our new dimension is given by the equation,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1aDA2uxLcBhOHRyvic9E0ok6QJVx6XQfK"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (7)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">And the hyperplane must satisfy,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=12st4zX1rwVUo_q9Ih7DDKe4P3MzL5NsI"><span class="c3">&nbsp;for positive samples, i.e. when </span><img src="https://drive.google.com/uc?export=view&id=1wFYQxlHGjyy24I_WM31I5UN1hdC-OZIB"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;..(8)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1ymtLRDjPrmnijHvFMiFfQXa3lsHucRh9"><span class="c3">&nbsp;for the negative samples, i.e. when </span><img src="https://drive.google.com/uc?export=view&id=13WM154WMsF5_eD8r-GHqBdOoeJkUGv5w"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.(9)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Here </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c1">&nbsp;maps our independent values to the new space with greater dimension in which our dataset turns to be linearly separable.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">To summarize the above two inequalities we can write,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1NhkeU8aFSoMpz5brn37zy9X5c3HqNme2"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (10)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">In real world problems it can happen that even in the new space the dataset is not linearly separable. To counter this problem we introduce a variable, </span><img src="https://drive.google.com/uc?export=view&id=1ZrTBM3gXm0rPWC7zEBiB_sR9XwVIqBNR"><span class="c1">&nbsp;as a tolerance margin in the classification thresholds, making the classifier more flexible in accepting possible errors. Now &nbsp;the hyperplane condition in Eq. (10) becomes Eq. (11), and the problem of finding the optimal hyperplane becomes a convex optimization problem given by Eq. (12). In this equation, C is the adjustment parameter for the edge of the hyperplane with the smallest possible misclassification, under the conditions of Eq. (11).</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1iKJdmpRsRLyoZEzFsajA2ype9uJdoEVq"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (11)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1N43EO4bjU6qM6wVZtQ_rl8nw-FIMY_zB"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.... (12)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Now we come to SVR. It uses principles similar to SVM, but the response variable is a continuous value </span><img src="https://drive.google.com/uc?export=view&id=1nsgwJ7ntwaTywFlWpcRz5F6H5VN1HMkl"><span class="c3">. Instead of seeking for the hyperplane in Eq. (11), SVR seeks the linear regression function, given by Eq. (13). To achieve this, a threshold error &epsilon; is defined to be minimized in the expression in Equation (14). This expression is called the &epsilon;-insensitivity loss error function. The SVR regression process therefore seeks to minimize &epsilon; in Eq. (14) and </span><img src="https://drive.google.com/uc?export=view&id=1u-UagvNlkUgl374MoVofWmlGrgQDzL91"><span class="c1">&nbsp;in the expression of R, defined in Eq. (15).</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1zJC5Aj3on5D7vjG4CyOGxH2e0UvGj6OM"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (13)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1ecBzHDzK7jJm6do_bbVhiXNstCVdyXUh"><span class="c1">&nbsp;&hellip;&hellip;. (14)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1AjOS8lEa2ne13kff9EIDFTZVI8gTWfRC"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. (15)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Again introducing the tolerance variables here as well, &nbsp;defining </span><img src="https://drive.google.com/uc?export=view&id=1jnpWCuITmMwDol8rUpQX1jtKECFfl0Ro"><span class="c3">&nbsp;as the value in excess of &epsilon; and </span><img src="https://drive.google.com/uc?export=view&id=10ZrfxIrOYcGDSFKZ8tmJjOlgY9royWmQ"><span class="c3">&nbsp;to limit the value to the regression target. Thus, the minimization of Eq. (15) becomes Eq. (16), under the conditions of Eqs. (17) and (18) for </span><img src="https://drive.google.com/uc?export=view&id=1y4okkqF3oWJPWv-D-4XZdFunohhGa2l_"><span class="c3">&nbsp;and </span><img src="https://drive.google.com/uc?export=view&id=1vOM-S_UO9DiJt-vUZ9QWLV7Fk9s1mQ2F"><span class="c1">.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1nBcCQOHNEfwoO3OPo38EdlXalcCSlC84"><img src="https://drive.google.com/uc?export=view&id=1y4okkqF3oWJPWv-D-4XZdFunohhGa2l_"><span class="c3">+</span><img src="https://drive.google.com/uc?export=view&id=1OeK2RNrVv-5B8DihiTPIanmEHiXKtwHa"><span class="c1">) ........................................................... (16)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1uZ_8__J83OPK3tc4A3Sx4hP8PfnUxgYw"><img src="https://drive.google.com/uc?export=view&id=1y4okkqF3oWJPWv-D-4XZdFunohhGa2l_"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip; (17)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1UOZK54Ld1sIWMWFJCKT8GJrV6IKSC6Ez"><img src="https://drive.google.com/uc?export=view&id=1OwZR8A2qEQOiI8ZYnxTm0TK0hxIw0h3m"><img src="https://drive.google.com/uc?export=view&id=1OeK2RNrVv-5B8DihiTPIanmEHiXKtwHa"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (18)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now let&rsquo;s focus on the Kernel Function now. It is,</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">&nbsp;</span><img src="https://drive.google.com/uc?export=view&id=1SrkAaS6ldHa9G3qjKuIoUbN-vOvTwvye"><span class="c1">&nbsp;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (19)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">So, it is the dot product of the </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c1">&nbsp;images of the vectors in our current space.</span></p><p class="c0"><span class="c3">The perks of kernels is that we can get the dot products of the </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c3">images without even knowing anything about the map </span><img src="https://drive.google.com/uc?export=view&id=1Os-7hzIIGpVjS91mVr9Gt0WceYks0u5B"><span class="c1">.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">There are mainly four types of kernel function in the SVM algorithm, namely, linear, radial basis function (RBF) Eq.(20), polynomial Eq.(21) and sigmoid function Eq.(22). In this project we have used RBF, polynomial and sigmoid functions and have compared the results.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1c7XsX8G2X6nFBi7Et4zzZtgzsg0OAqcz"><span class="c1">where gamma is a parameter &hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (20)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1oprInpHu6IDdaVZWs1jy3bJupZ4YSEOH"><span class="c1">&nbsp;where d and r are parameters &hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.. (21)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><img src="https://drive.google.com/uc?export=view&id=1icL7ETHxhDrP5ofLn_yCSzWB_DhWCHtT"><span class="c3">&nbsp;where </span><img src="https://drive.google.com/uc?export=view&id=1uMt_813TkEniUq14K-IIQegOdV5qZ6lp"><span class="c1">&nbsp;and r are parameters &hellip;&hellip;&hellip;&hellip;&hellip;... (22)</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">In the RBF kernel, </span><img src="https://drive.google.com/uc?export=view&id=1gFxtmppjKtnxNi6m2gi7234xK4_ifyg0"><span class="c3">&nbsp;is the squared Euclidean distance between two feature vector and </span><img src="https://drive.google.com/uc?export=view&id=1uMt_813TkEniUq14K-IIQegOdV5qZ6lp"><span class="c1">is a parameter which determines how much influence a single training data point has. RBF kernel is a function whose value depends on the distance from the origin or from some point.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">In the polynomial kernel, d is the degree of the kernel and r is a constant term. Here, we simply calculate the dot product by increasing the power of the kernel.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">An interesting fact to be noted is that the shape of the kernel function directly influences the values obtained by the SVR. Similarly, the constant c in Eq. (16) and the parameters </span><img src="https://drive.google.com/uc?export=view&id=1uMt_813TkEniUq14K-IIQegOdV5qZ6lp"><span class="c1">&nbsp;and d in Eqs. (21) and (22) should be optimized.</span></p><p class="c0 c4"><span class="c1"></span></p>

<p class="c0"><span class="c14"><b>2.Experiment and Results:</b></span></p><p class="c0 c4"><span class="c23 c14 c20"></span></p><ol class="c5 lst-kix_list_5-0 start" start="1"><li class="c0 c8 li-bullet-0"><span class="c17 c3"><u>Data Set</u></span><span class="c1">: We have </span><span class="c3">taken the Uniqlo</span><span class="c1">&nbsp;(Fast Retailing ) in Tokyo Stock Exchange dataset available on Kaggle. The dataset contains data of 5 years i.e 2012-2016. This is how our data looks like.</span></li></ol><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 480.00px; height: 164.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1YJMOO28ci-tcAeiBRFJA2rWaQ19YqUNh" style="width: 480.00px; height: 164.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c4"><span class="c1"></span></p><ol class="c5 lst-kix_list_5-0" start="2"><li class="c0 c8 li-bullet-0"><span class="c3 c17"><u>Preprocessing Data and Data Analysis:</u></span><span class="c1">&nbsp;We only need to predict the &lsquo;Close Price&rsquo; with respect to Date. After checking whether there </span><span class="c3">exists a null</span><span class="c1">&nbsp;value in the date column or not, we changed the data type of the Date column from string to date-time. Then we compared the month-wise opening and closing price of actual data.</span></li></ol><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 676.16px; height: 372.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1kymAJ4Nie23eU4tqWVhXjhKJq2tYowtp" style="width: 676.16px; height: 372.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">Using plotly we got a figure of opening and close price of actual data vs Date.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 658.63px; height: 348.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1RXNZLaMy0YUk4RA0gD-KnJTj44aGYzgS" style="width: 658.63px; height: 348.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c5 lst-kix_list_5-0" start="3"><li class="c0 c8 li-bullet-0"><span class="c17 c3"><u>Close Price Prediction:</u></span><span class="c1">&nbsp;We made a separate data frame with Date and Close as columns and took a copy of the same for further use. After normalizing the feature(close value) into range &nbsp;0 to 1, we split the data into 70% training data and 30% testing data. Using different kernels (RBF, Sigmoid, Polynomial(degree=2)) of SVR from Sci-kit Learn we got the following graphs as result.</span></li></ol><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 636.00px; height: 321.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1tKCm5A4HFme0GNagtYh6TJkIqHheTEFX" style="width: 636.00px; height: 321.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 644.00px; height: 280.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1x5frJj1W_SLv1uP4JYM7jSCN9cAxZVYo" style="width: 644.00px; height: 280.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 648.00px; height: 320.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1GacXc6fLSLLoYnt-bTti05iiFWIfHsja" style="width: 648.00px; height: 320.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3 c20">After getting the model we will predict the close price using different kernels. Here it is shown </span><span class="c3">for the next</span><span class="c1">&nbsp;30 days.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 649.24px; height: 315.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1vjFKilU0DZHasB95BQp2adn8tGJUioEk" style="width: 649.24px; height: 315.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">Now, the question is which kernel we should use for better result. For this we will use metrics like RMSE or MSE and R-squared measure. MSE of the predicted values is calculated for each kernel function, and is compared with each other.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">MSE = </span><img src="https://drive.google.com/uc?export=view&id=15Gp759N4_6F6FfKQBCkr6L42cevn8UiJ"></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c3">Where, </span><img src="https://drive.google.com/uc?export=view&id=131SZpvytdW_NKcLOI_eRsqWeOIw_WZV0"><span class="c1">&rsquo;s are real sample values.</span></p><p class="c0 c24"><span class="c3">&nbsp;</span><img src="https://drive.google.com/uc?export=view&id=1StfhKyiQEQ1nDCPl1XmHrJKpHGJipq76"><span class="c1">&rsquo;s are predicted values from our SVR model.</span></p><p class="c0"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">&nbsp;T is the total number of test samples. &nbsp;</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 657.00px; height: 278.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=14WSXTyEsNvMyLjaQas-dY_UM2KoU1xVA" style="width: 657.00px; height: 278.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now let&rsquo;s look into a statistical measure called the R-squared.</span></p><p class="c0"><span class="c3">It </span><span class="c3 c21">represents the proportion of the variance for a dependent variable that&#39;s explained by an independent variable or variables in a regression model. </span><span class="c1">&nbsp;We have calculated the R^2 value(R) for each kernel. We know that for R=1, it is considered as a best model and for R=0 or &lt; 0 ; it is considered as a worse model.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 659.00px; height: 335.00px;"><img alt="" src="https://drive.google.com/uc?export=view&id=1x1nRqYBk5m5r5HmZXdEx83YTevscTv3F" style="width: 659.00px; height: 335.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1">Considering the above two measures, we can clearly see that &lsquo;Sigmoid&rsquo; is a very bad kernel option for our dataset. That&rsquo;s why we haven&rsquo;t considered it for the &lsquo;30 days prediction&rsquo;.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">Now, comparing the above statistical measures for RBF and Polynomial kernel, we can conclude that for our experiment RBF kernel is the best model.</span></p><p class="c0 c4"><span class="c1"></span></p>

<p class="c0"><span class="c17 c14"><b>3.Reference:</b></span></p><ol class="c5 lst-kix_list_3-0 start" start="1"><li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Vanukuru K.S.R, &quot;Stock Market Prediction Using Machine Learning&quot;, Vol. 05 Issue. 10, International Research Journal of Engineering and Technology (IRJET), Oct 2018. </span><span class="c11 c3"><a class="c10" href="https://www.google.com/url?q=http://dx.doi.org/10.13140/RG.2.2.12300.77448&amp;sa=D&amp;source=editors&amp;ust=1634328078768000&amp;usg=AOvVaw2DvZnkKNmVEY3yLLBwS5Kx">Link</a></i></span><span class="c9 c3">. </span></li><li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Henrique B.M, Sobreiro V.A and Kimura H, Stock Price Prediction Using Support Vector Regression on Daily and Up to the Minute Prices, The Journal of Finance and Data Science, vol. 4, no. 3, pp. 183-201, 2018. </span><span class="c11 c3"><a class="c10" href="https://www.google.com/url?q=https://doi.org/10.1016/j.jfds.2018.04.003&amp;sa=D&amp;source=editors&amp;ust=1634328078769000&amp;usg=AOvVaw25SLJBiIBwADiuSTms5Jp3">Link</a></i></span><span class="c3 c9">.</span></li><li class="c0 c8 li-bullet-0"><span class="c9 c3"><i>Nanda M.A, Seminar K.B, Nandika D and Maddu A, &ldquo;</span><span class="c3 c12">A Comparison Study of Kernel Functions in the Support Vector Machine and Its Application for Termite Detection</span><span class="c9 c3">&rdquo;, Information (Switzerland), 9(1), 5, Jan 2018. </span><span class="c3 c11"><a class="c10" href="https://www.google.com/url?q=https://doi.org/10.3390/info9010005&amp;sa=D&amp;source=editors&amp;ust=1634328078769000&amp;usg=AOvVaw0bhet3s8-16m_DRNtpdUrc">Link.</a><i></span></li></ol></body></html>